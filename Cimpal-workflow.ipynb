{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f306301d-4935-4c08-99f1-08cf24ba6db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07897018-a033-4d20-83f5-5d5cf9d4c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_s3_server = \"scruffy.lab.uvalight.net:9000\"\n",
    "param_s3_access_key = \"vl-biodt-demo\"\n",
    "# Add the secret key to run the notebook\n",
    "param_s3_secret_key = \"\"\n",
    "\n",
    "param_s3_bucket_input = \"naa-vre-public\"\n",
    "param_s3_prefix_input = \"vl-biodt-demo/\"\n",
    "\n",
    "param_s3_bucket_output = \"naa-vre-user-data\"\n",
    "param_s3_prefix_output = \"vl-biodt-demo/\"\n",
    "\n",
    "conf_data_dir = '/tmp/data'\n",
    "\n",
    "param_grid_size_para = 25\n",
    "\n",
    "# Zonalfilter\n",
    "param_zone_field_para= \"id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf0c86a-546f-4a26-ba14-a95e9ca3d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimpal-get-files\n",
    "\n",
    "import os\n",
    "from minio import Minio\n",
    "\n",
    "\n",
    "minio_client = Minio(param_s3_server, access_key=param_s3_access_key, secret_key=param_s3_secret_key, secure=True)\n",
    "\n",
    "for item in minio_client.list_objects(param_s3_bucket_input, prefix=f\"{param_s3_prefix_input}\", recursive=True):\n",
    "    target_file = f\"{conf_data_dir}/input/{item.object_name.removeprefix(param_s3_prefix_input)}\"\n",
    "    if not os.path.exists(target_file):\n",
    "        print(\"Downloading\", item.object_name)\n",
    "        minio_client.fget_object(param_s3_bucket_input, item.object_name, target_file)\n",
    "\n",
    "occ_taxa = f\"{conf_data_dir}/input/Cimpal_resources\"\n",
    "biotope_shp_path_file = f\"{conf_data_dir}/input/Cimpal_resources\"\n",
    "weight_file = f\"{conf_data_dir}/input/Cimpal_resources/weight_wp.csv\"\n",
    "pathway_file = f\"{conf_data_dir}/input/Cimpal_resources/CIMPAL_paths.csv\"\n",
    "zones_file = f\"{conf_data_dir}/input/zones\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9524d22b-0c63-4113-9164-863b39e932ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cimpal-calculator\n",
    "# ---\n",
    "# NaaVRE:\n",
    "#  cell:\n",
    "#   inputs:\n",
    "#    - occ_taxa: String\n",
    "#    - biotope_shp_path_file: String\n",
    "#    - weight_file: String\n",
    "#    - pathway_file: String\n",
    "#   outputs:\n",
    "#    - out_path: String\n",
    "# ...\n",
    "\n",
    "import os\n",
    "\n",
    "# inputs\n",
    "occ_and_taxa_path = occ_taxa\n",
    "biotope_shp_path = biotope_shp_path_file\n",
    "weights_path = weight_file\n",
    "pathways_path = pathway_file\n",
    "\n",
    "# outputs\n",
    "os.makedirs(f\"{conf_data_dir}/output/Cimpal\")\n",
    "out_path = f\"{conf_data_dir}/output/Cimpal\"\n",
    "\n",
    "\n",
    "\n",
    "import glob, subprocess, pandas\n",
    "import os, re, csv\n",
    "\n",
    "from osgeo import ogr, osr\n",
    "\n",
    "from osgeo import gdal\n",
    "\n",
    "import pyproj\n",
    "from shapely.geometry import MultiPolygon, Polygon, Point\n",
    "from shapely.ops import transform\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "\n",
    "import argparse\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Biotope Cimpal Calculator\")\n",
    "\n",
    "# parser.add_argument(\n",
    "#     \"-ot\",\n",
    "#     \"--occ_and_taxa\",\n",
    "#     type=str,\n",
    "#     help=\"Occurrences And Alien Taxonomy Dir\",\n",
    "#     required=True,\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"-bshp\", \"--biotope_shp\", type=str, help=\"Biotope's Shp Dir\", required=True\n",
    "# )\n",
    "# parser.add_argument(\n",
    "#     \"-p\", \"--pathways\", type=str, help=\"Pathways csv file\", required=False\n",
    "# )\n",
    "# parser.add_argument(\"-w\", \"--weights\", type=str, help=\"Weights csv file\", required=True)\n",
    "# parser.add_argument(\n",
    "#     \"-eea\", \"--useEEA\", type=bool, help=\"Use EEA projection\", required=True\n",
    "# )\n",
    "# parser.add_argument(\"-grid\", \"--grid_size\", type=int, help=\"Grid Size\", required=True)\n",
    "# parser.add_argument(\"-o\", \"--out\", type=str, help=\"outpath\", required=True)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "gbif_dir = Path(occ_and_taxa_path)\n",
    "shp_dir = biotope_shp_path\n",
    "shp_files = os.listdir(shp_dir)\n",
    "impact_matrix = weights_path\n",
    "grid_size = param_grid_size_para\n",
    "nis_pathways_matrix_path_dir = pathways_path\n",
    "\n",
    "# occurences = \"/export/projects/LIFEWATCH/cimpal/metz/occur_RiaAvei_Pinheiro2019_LWformat_2/occur_RiaAvei_Pinheiro2019_LWformat_2.csv\"\n",
    "# alien_taxa_csv = (\n",
    "#     \"/export/projects/LIFEWATCH/cimpal/metz/Alien_taxa_List/\" + \"Alien_taxa_List.csv\"\n",
    "# )\n",
    "\n",
    "# set the paths to the apps\n",
    "otbbin = \"/usr/local/otb/bin/\"\n",
    "outpath = out_path\n",
    "\n",
    "shp_file = None\n",
    "\n",
    "for file in shp_files:\n",
    "    if file.endswith(\".shp\"):\n",
    "        shp_file = os.path.join(shp_dir, file)\n",
    "        break\n",
    "\n",
    "if not shp_file:\n",
    "    raise FileNotFoundError(\"`.shp` file not found in zip\")\n",
    "\n",
    "# get input file names from orchestrator\n",
    "\n",
    "biotopes = str(shp_dir) + '/'\n",
    "outpath = out_path + '/'\n",
    "weight_matrix_template = weights_path\n",
    "filter_matrix_csv = pathways_path\n",
    "occurences = str(Path(gbif_dir, \"occurrence.txt\"))\n",
    "alienTaxa = str(Path(gbif_dir, \"alientaxa.txt\"))\n",
    "\n",
    "print(\"=================================\")\n",
    "print(f\"biotopes {biotopes}\")\n",
    "print(f\"outpath {outpath}\")\n",
    "print(f\"weight_matrix_template {weight_matrix_template}\")\n",
    "print(f\"filter_matrix_csv {filter_matrix_csv}\")\n",
    "print(f\"occurences {occurences}\")\n",
    "print(f\"alienTaxa {alienTaxa}\")\n",
    "print(\"=================================\")\n",
    "\n",
    "\n",
    "\n",
    "# # predefined variables\n",
    "zone_field = 'id_habitat'\n",
    "field_name_oc = \"scientificName\"\n",
    "useEEA = True\n",
    "LAEA=True\n",
    "grid_size = int(param_grid_size_para)\n",
    "field_name_radius = \"dispersionRadius\"\n",
    "radius_default = 100\n",
    "field_name = \"Habitat\"\n",
    "\n",
    "## A. Convert all data about habitat into raster layers on a single grid\n",
    "## A.1 Check that all data are in the same coordinate system and compute the maximum extent\n",
    "##\n",
    "extent_tot = [\n",
    "    100000000,\n",
    "    -100000000,\n",
    "    100000000,\n",
    "    -100000000,\n",
    "]  # initialise extent: xmin, xmax, ymin, ymax\n",
    "\n",
    "if useEEA:\n",
    "    shp_candidates = glob.glob(biotopes + \"[sS]ingle_*.shp\") + glob.glob(\n",
    "        biotopes + \"[Ll]ist_*.shp\"\n",
    "    )\n",
    "    print(len(shp_candidates))\n",
    "    for shp in [x for x in shp_candidates if not \"LAEA\" in x]:\n",
    "        # reproject the dataset\n",
    "        print(\"reprojecting \" + shp)\n",
    "        outshp = shp[:-4] + \"_LAEA.shp\"\n",
    "        subprocess.call([\"ogr2ogr\", \"-t_srs\", \"EPSG:3035\", outshp, shp])\n",
    "        driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "        dataSource = driver.Open(outshp, 0)  # 0 means read-only. 1 means writeable.\n",
    "\n",
    "        # Check to see if shapefile is found.\n",
    "        if dataSource is None:\n",
    "            print(\"Could not open %s\" % (shp))\n",
    "        else:\n",
    "            inLayer = dataSource.GetLayer()\n",
    "            extent = inLayer.GetExtent()\n",
    "            if extent[0] < extent_tot[0]:\n",
    "                extent_tot[0] = extent[0]\n",
    "            if extent[1] > extent_tot[1]:\n",
    "                extent_tot[1] = extent[1]\n",
    "            if extent[2] < extent_tot[2]:\n",
    "                extent_tot[2] = extent[2]\n",
    "            if extent[3] > extent_tot[3]:\n",
    "                extent_tot[3] = extent[3]\n",
    "    grid_srs = osr.SpatialReference()\n",
    "    grid_srs.ImportFromEPSG(3035)\n",
    "else:\n",
    "    srs_list = []\n",
    "    refdict = {}\n",
    "    for shp in glob.glob(biotopes + \"[Ss]ingle_*.shp\")  + glob.glob(\n",
    "        biotopes + \"[Ll]ist_*.shp\"\n",
    "    ):\n",
    "        driver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "        dataSource = driver.Open(shp, 0)  # 0 means read-only. 1 means writeable.\n",
    "        # Check to see if shapefile is found.\n",
    "        if dataSource is None:\n",
    "            print(\"Could not open %s\" % (shp))\n",
    "        else:\n",
    "            inLayer = dataSource.GetLayer()\n",
    "            refdict[shp] = inLayer.GetSpatialRef()\n",
    "            srs_list.append(inLayer.GetSpatialRef())\n",
    "            extent = inLayer.GetExtent()\n",
    "            if extent[0] < extent_tot[0]:\n",
    "                extent_tot[0] = extent[0]\n",
    "            if extent[1] > extent_tot[1]:\n",
    "                extent_tot[1] = extent[1]\n",
    "            if extent[2] < extent_tot[2]:\n",
    "                extent_tot[2] = extent[2]\n",
    "            if extent[3] > extent_tot[3]:\n",
    "                extent_tot[3] = extent[3]\n",
    "    grid_srs = srs_list[0]\n",
    "    for my_srs in srs_list:\n",
    "        if my_srs.IsSame(grid_srs):\n",
    "            print(\"OK\")\n",
    "        else:\n",
    "            print(\"all data must be in the same coordinate system\")\n",
    "            for layer_ref in refdict.items():\n",
    "                print(\n",
    "                    layer_ref[0]\n",
    "                    + \" has coordinate system \"\n",
    "                    + layer_ref[1].ExportToPrettyWkt()\n",
    "                )\n",
    "            sys.exit()\n",
    "\n",
    "print(\"Full extent based on input files\")\n",
    "print(extent_tot)\n",
    "x_min, x_max, y_min, y_max = extent_tot\n",
    "\n",
    "\n",
    "x_min = ((x_min // grid_size) - 1) * grid_size\n",
    "y_min = ((y_min // grid_size) - 1) * grid_size\n",
    "x_max = ((x_max // grid_size) + 1) * grid_size\n",
    "y_max = ((y_max // grid_size) + 1) * grid_size\n",
    "\n",
    "\n",
    "for shp in glob.glob(biotopes + \"[Ll]ist_*.shp\"):\n",
    "    dataSource = driver.Open(shp, 0)\n",
    "    layer = dataSource.GetLayer()\n",
    "    values_list = []\n",
    "    for feature in layer:\n",
    "        newval = feature.GetField(field_name)\n",
    "        values_list.append(newval)\n",
    "    layer.ResetReading()\n",
    "    for fieldval in list(set(values_list)):\n",
    "        layer.SetAttributeFilter(\"{0} = '{1}'\".format(field_name, fieldval))\n",
    "        # Create the output LayerS\n",
    "        filename = re.sub(\"[^a-zA-Z0-9\\n\\.]\", \"_\", fieldval).lower()\n",
    "        print(filename)\n",
    "        outShapefile = os.path.join(\n",
    "            os.path.split(shp)[0], \"single_{}.shp\".format(filename)\n",
    "        )\n",
    "        outDriver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "        # Remove output shapefile if it already exists\n",
    "        if os.path.exists(outShapefile):\n",
    "            outDriver.DeleteDataSource(outShapefile)\n",
    "        # Create the output shapefile\n",
    "        outDataSource = outDriver.CreateDataSource(outShapefile)\n",
    "        out_lyr_name = os.path.splitext(os.path.split(outShapefile)[1])[0]\n",
    "        outLayer = outDataSource.CreateLayer(\n",
    "            out_lyr_name, grid_srs, geom_type=ogr.wkbMultiPolygon\n",
    "        )\n",
    "        # Get the output Layer's Feature Definition\n",
    "        outLayerDefn = outLayer.GetLayerDefn()\n",
    "        # Add features to the ouput Layer\n",
    "        for inFeature in layer:\n",
    "            # Create output Feature\n",
    "            outFeature = ogr.Feature(outLayerDefn)\n",
    "            # Set geometry as centroid\n",
    "            geom = inFeature.GetGeometryRef()\n",
    "            outFeature.SetGeometry(geom.Clone())\n",
    "            # Add new feature to output Layer\n",
    "            outLayer.CreateFeature(outFeature)\n",
    "        # Close DataSources\n",
    "        layer.ResetReading()\n",
    "        outDataSource.Destroy()\n",
    "\n",
    "if LAEA:\n",
    "    biotopes_proj = biotopes + \"[Ss]ingle*_LAEA.shp\"\n",
    "else:\n",
    "    biotopes_proj = biotopes + \"[Ss]ingle*.shp\"\n",
    "for shp in glob.glob(biotopes_proj):\n",
    "    \n",
    "    # Define grid_size and NoData value of new raster\n",
    "    NoData_value = 0\n",
    "    # Filename of the raster Tiff that will be created\n",
    "    norm_name = re.sub(\"[^a-zA-Z0-9\\n\\.]\", \"_\", os.path.split(shp)[1])\n",
    "    out_name = norm_name[:-4].replace(\"single_\", \"h_\") + \".tif\"\n",
    "    raster_fn = os.path.join(os.path.split(shp)[0], out_name)\n",
    "    # Open the data source and read in the extent\n",
    "    source_ds = ogr.Open(shp)\n",
    "    source_layer = source_ds.GetLayer()\n",
    "    source_srs = source_layer.GetSpatialRef()\n",
    "    # Create the destination data source\n",
    "    x_res = int((x_max - x_min) / grid_size)\n",
    "    y_res = int((y_max - y_min) / grid_size)\n",
    "    target_ds = gdal.GetDriverByName(\"GTiff\").Create(\n",
    "        raster_fn, x_res, y_res, gdal.GDT_Byte\n",
    "    )\n",
    "    target_ds.SetGeoTransform((x_min, grid_size, 0, y_max, 0, -grid_size))\n",
    "    band = target_ds.GetRasterBand(1)\n",
    "    band.SetNoDataValue(NoData_value)\n",
    "    target_ds.SetProjection(source_srs.ExportToWkt())\n",
    "    # Rasterize\n",
    "    gdal.RasterizeLayer(\n",
    "        target_ds, [1], source_layer, burn_values=[1], options=[\"ALL_TOUCHED=TRUE\"]\n",
    "    )\n",
    "\n",
    "##\n",
    "#### B create raster for each species based on an uncertainty radius\n",
    "##\n",
    "wgs84_globe = pyproj.Proj(proj=\"latlong\", ellps=\"WGS84\")\n",
    "\n",
    "# function for a geodetic buffer around a point\n",
    "def point_buff_geodetic(c1, c2, radius, in_proj, out_proj):\n",
    "    # get the equidistant projection centered on the point\n",
    "    _lon, _lat = pyproj.transform(in_proj, wgs84_globe, c1, c2)\n",
    "    aeqd = pyproj.Proj(\n",
    "        proj=\"aeqd\", ellps=\"WGS84\", datum=\"WGS84\", lat_0=_lat, lon_0=_lon\n",
    "    )\n",
    "    # print(aeqd)\n",
    "    # buffer in azimuthal equidistant projection and project it to its initial srs\n",
    "    return sh_transform(\n",
    "        partial(pyproj.transform, aeqd, out_proj), Point(0, 0).buffer(radius)\n",
    "    )\n",
    "\n",
    "def point_buff_geodetic_wgs(c1, c2, radius):\n",
    "    # get the equidistant projection centered on the point\n",
    "    aeqd = pyproj.Proj(proj=\"aeqd\", ellps=\"WGS84\", datum=\"WGS84\", lat_0=c2, lon_0=c1)\n",
    "    # print(\"internal longitude = \"  + str(c1))\n",
    "    # print(aeqd)\n",
    "    # buffer in azimuthal equidistant projection and project it to its initial srs\n",
    "    proj4str = \"+proj=aeqd +lat_0=%s +lon_0=%s +x_0=0 +y_0=0\" % (c2, c1)\n",
    "    aeqd = pyproj.Proj(proj4str)  # azimuthal equidistant\n",
    "    project = partial(pyproj.transform, aeqd, pyproj.Proj(\"EPSG:4326\"), always_xy=True)\n",
    "    return transform(project, Point(0, 0).buffer(radius))\n",
    "\n",
    "\n",
    "###B.1 get the list of all species\n",
    "print(grid_srs.ExportToWkt())\n",
    "hab_proj = pyproj.Proj(grid_srs.ExportToWkt())\n",
    "if \".shp\" in occurences:\n",
    "    dataSource = driver.Open(occurences, 0)\n",
    "    sp_layer = dataSource.GetLayer()\n",
    "    field_name_oc = \"scientific\"\n",
    "    field_name_radius = \"radius\"\n",
    "    sp_srs = sp_layer.GetSpatialRef()\n",
    "    sp_proj = pyproj.Proj(sp_srs.ExportToWkt())\n",
    "    species_list = []\n",
    "    for feature in sp_layer:\n",
    "        newval = feature.GetField(field_name_oc)\n",
    "        if not newval in species_list:\n",
    "            species_list.append(newval)\n",
    "    sp_layer.ResetReading()\n",
    "    print(\"there are \" + str(len(species_list)) + \" items\")\n",
    "\n",
    "    for fieldval in species_list:\n",
    "        sp_layer.SetAttributeFilter(\"{0} = '{1}'\".format(field_name_oc, fieldval))\n",
    "        # Create the output LayerS\n",
    "        filename = re.sub(\"[^a-zA-Z0-9\\n\\.]\", \"_\", fieldval).lower()\n",
    "        print(filename)\n",
    "        outShapefile = os.path.join(\n",
    "            os.path.split(occurences)[0], \"spf_{}.shp\".format(filename)\n",
    "        )\n",
    "        outDriver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "        # Remove output shapefile if it already exists\n",
    "        if os.path.exists(outShapefile):\n",
    "            outDriver.DeleteDataSource(outShapefile)\n",
    "        # Create the output shapefile\n",
    "        outDataSource = outDriver.CreateDataSource(outShapefile)\n",
    "        out_lyr_name = os.path.splitext(os.path.split(outShapefile)[1])[0]\n",
    "        outLayer = outDataSource.CreateLayer(\n",
    "            out_lyr_name, grid_srs, geom_type=ogr.wkbMultiPolygon\n",
    "        )\n",
    "        # Get the output Layer's Feature Definition\n",
    "        outLayerDefn = outLayer.GetLayerDefn()\n",
    "        # print(outLayerDefn)\n",
    "        # Add features to the ouput Layer\n",
    "        for inFeature in sp_layer:\n",
    "            # Create output Feature\n",
    "            outFeature = ogr.Feature(outLayerDefn)\n",
    "            # Set geometry as centroid\n",
    "            geom = inFeature.GetGeometryRef()\n",
    "            uncertainty_radius = inFeature.GetField(field_name_radius)\n",
    "            geom_out = point_buff_geodetic(\n",
    "                geom.GetX(), geom.GetY(), uncertainty_radius, sp_proj, hab_proj\n",
    "            )\n",
    "            poly = ogr.CreateGeometryFromWkt(geom_out.wkt)\n",
    "            # print(poly)\n",
    "            multipolygon = ogr.Geometry(ogr.wkbMultiPolygon)\n",
    "            multipolygon.AddGeometry(poly)\n",
    "            outFeature.SetGeometry(multipolygon)\n",
    "            # Add new feature to output Layer\n",
    "            outLayer.CreateFeature(outFeature)\n",
    "            # inFeature.Destroy()\n",
    "        # Close DataSources\n",
    "        sp_layer.ResetReading()\n",
    "        outDataSource.Destroy()\n",
    "elif \".txt\" in occurences:\n",
    "    radius = 50\n",
    "    print(\"csv input\")\n",
    "    # get a list of alien taxa for filtering\n",
    "    alien_taxa = {}\n",
    "    with open(alienTaxa, newline=\"\") as csvfile_at:\n",
    "        atl = csv.reader(csvfile_at, delimiter=\"\\t\", quotechar=\"|\")\n",
    "        i = 0\n",
    "        for row in atl:\n",
    "            if i == 0:\n",
    "                fld_idx = row.index(\"scientificName\")\n",
    "                try:\n",
    "                    radius_idx = row.index(\"dispersionRadius\")\n",
    "                except:\n",
    "                    radius_idx = -1\n",
    "                i += 1\n",
    "            else:\n",
    "                if radius_idx < 0:\n",
    "                    alien_taxa[row[fld_idx]] = radius_default\n",
    "                else:\n",
    "                    alien_taxa[row[fld_idx]] = float(row[radius_idx])\n",
    "    print(\"there are \" + str(len(alien_taxa)) + \" items\")\n",
    "    df_occs = pandas.read_csv(\n",
    "        occurences,\n",
    "        delimiter=\"\\t\",\n",
    "        usecols=[\n",
    "            field_name_oc,\n",
    "            \"decimalLatitude\",#-\"NCoord\",\n",
    "            \"decimalLongitude\",#-\"ECoord\",\n",
    "        ],\n",
    "    )\n",
    "    sp_out_srs = osr.SpatialReference()\n",
    "    sp_out_srs.ImportFromEPSG(4326)\n",
    "    for alien_sp in alien_taxa:\n",
    "        # get the info from the dict\n",
    "        print(alien_sp + \" \" + str(round(alien_taxa[alien_sp], 0)))\n",
    "        dis_radius = alien_taxa[alien_sp]\n",
    "        df_temp = df_occs.loc[df_occs[field_name_oc] == alien_sp]\n",
    "        filename = re.sub(\"[^a-zA-Z0-9\\n\\.]\", \"_\", alien_sp).lower()\n",
    "        outShapefile_csv = os.path.join(\n",
    "            os.path.split(occurences)[0], \"sp_{}.shp\".format(filename)\n",
    "        )\n",
    "        outDriver = ogr.GetDriverByName(\"ESRI Shapefile\")\n",
    "        # Remove output shapefile if it already exists\n",
    "        if os.path.exists(outShapefile_csv):\n",
    "            outDriver.DeleteDataSource(outShapefile_csv)\n",
    "        # Create the output shapefile\n",
    "        outDataSource_csv = outDriver.CreateDataSource(outShapefile_csv)\n",
    "        out_lyr_name_csv = os.path.splitext(os.path.split(outShapefile_csv)[1])[0]\n",
    "        outLayer_csv = outDataSource_csv.CreateLayer(\n",
    "            out_lyr_name_csv, sp_out_srs, geom_type=ogr.wkbMultiPolygon\n",
    "        )\n",
    "        # Get the output Layer's Feature Definition\n",
    "        outLayerDefn_csv = outLayer_csv.GetLayerDefn()\n",
    "        # print(outLayerDefn)\n",
    "        # Add features to the ouput Layer\n",
    "        j = 0\n",
    "        for i, row in df_temp.iterrows():\n",
    "            # Create output Feature\n",
    "            outFeature_csv = ogr.Feature(outLayerDefn_csv)\n",
    "            # Set geometry coordinates\n",
    "            geom_out = point_buff_geodetic_wgs(\n",
    "                #-float(row.NCoord), float(row.ECoord), dis_radius\n",
    "                float(row.decimalLongitude) ,float(row.decimalLatitude),dis_radius\n",
    "            )  # float(row.DispersionRadius))\n",
    "            poly = ogr.CreateGeometryFromWkt(geom_out.wkt)\n",
    "            # print(poly)\n",
    "            multipolygon = ogr.Geometry(ogr.wkbMultiPolygon)\n",
    "            multipolygon.AddGeometry(poly)\n",
    "            outFeature_csv.SetGeometry(multipolygon)\n",
    "            # Add new feature to output Layer\n",
    "            outLayer_csv.CreateFeature(outFeature_csv)\n",
    "            j += 1\n",
    "            # inFeature.Destroy()\n",
    "        # Close DataSources\n",
    "        print(filename + \" has\" + str(j) + \" features\")\n",
    "        outDataSource_csv.Destroy()\n",
    "        grid_srs.AutoIdentifyEPSG()\n",
    "        subprocess.call(\n",
    "            [\n",
    "                \"ogr2ogr\",\n",
    "                \"-t_srs\",\n",
    "                \"EPSG:\" + str(grid_srs.GetAuthorityCode(None)),\n",
    "                outShapefile_csv[:-4] + \"_loc.shp\",\n",
    "                outShapefile_csv,\n",
    "            ]\n",
    "        )\n",
    "else:\n",
    "    print(\"No valid imput for species occurences\")\n",
    "\n",
    "\n",
    "## C.2 rasterize the species occurrences (one file per species)\n",
    "\n",
    "for shp in glob.glob(os.path.join(os.path.split(occurences)[0], \"sp_*loc.shp\")):\n",
    "\n",
    "    # Define NoData value of new raster\n",
    "    NoData_value = 0\n",
    "\n",
    "    # Filename of the raster Tiff that will be created\n",
    "    raster_fn = shp[:-4].replace(\"sp_\", \"s_\") + \".tif\"\n",
    "    print(raster_fn + \" raster processing\")\n",
    "    # Open the data source and read in the extent\n",
    "    source_ds = ogr.Open(shp)\n",
    "    source_layer = source_ds.GetLayer()\n",
    "    source_srs = source_layer.GetSpatialRef()\n",
    "\n",
    "    # Create the destination data source\n",
    "    x_res = int((x_max - x_min) / grid_size)\n",
    "    y_res = int((y_max - y_min) / grid_size)\n",
    "    target_ds = gdal.GetDriverByName(\"GTiff\").Create(\n",
    "        raster_fn, x_res, y_res, gdal.GDT_Byte\n",
    "    )\n",
    "    target_ds.SetGeoTransform((x_min, grid_size, 0, y_max, 0, -grid_size))\n",
    "    band = target_ds.GetRasterBand(1)\n",
    "    band.SetNoDataValue(NoData_value)\n",
    "    target_ds.SetProjection(source_srs.ExportToWkt())\n",
    "    # Rasterize\n",
    "    gdal.RasterizeLayer(\n",
    "        target_ds, [1], source_layer, burn_values=[1], options=[\"ALL_TOUCHED=TRUE\"]\n",
    "    )\n",
    "\n",
    "\n",
    "# C compute CIMPAL value based on habitat and occurence files\n",
    "##\n",
    "\n",
    "for weight_matrix_csv in [weight_matrix_template]:\n",
    "    w = os.path.split(weight_matrix_csv)[1][:-4]\n",
    "    print(w)\n",
    "    with open(weight_matrix_csv, newline=\"\") as csvfile:\n",
    "        w_matrix = csv.reader(csvfile, delimiter=\"\\t\", quotechar=\"|\")\n",
    "        i = 0\n",
    "        for row in w_matrix:\n",
    "            print(f\"{row}\")\n",
    "            if i == 0:\n",
    "                habitat_names = [\n",
    "                    re.sub(\"[^a-zA-Z0-9\\n\\.]\", \"_\", x).lower() for x in row\n",
    "                ]\n",
    "                i += 1\n",
    "            else:\n",
    "                sp_name = re.sub(\"[^a-zA-Z0-9\\n\\.]\", \"_\", row[0]).lower()\n",
    "                sp_file = os.path.join(\n",
    "                    os.path.split(occurences)[0], \"s_{}_loc.tif\".format(sp_name)\n",
    "                )\n",
    "                if os.path.exists(sp_file):\n",
    "                    j = 0\n",
    "                    k = 1\n",
    "                    expression = []\n",
    "                    file_list = [sp_file]\n",
    "                    for habitat in row:\n",
    "                        if j > 0:\n",
    "                            if LAEA:\n",
    "                                hab_file = os.path.join(\n",
    "                                    biotopes, \"h_{}_LAEA.tif\".format(habitat_names[j])\n",
    "                                    )\n",
    "                            else:\n",
    "                                hab_file = os.path.join(\n",
    "                                    biotopes, \"h_{}.tif\".format(habitat_names[j])\n",
    "                                    )\n",
    "                            if os.path.exists(hab_file):\n",
    "                                if float(habitat) > 0:\n",
    "                                    k += 1\n",
    "                                    expression.append(\n",
    "                                        \"im1b1*im{0}b1*{1}\".format(k, habitat)\n",
    "                                    )\n",
    "                                    file_list.append(hab_file)\n",
    "                            else:\n",
    "                                print(\n",
    "                                    \"WARNING: no file for {}\".format(habitat_names[j])\n",
    "                                )\n",
    "                                expression.append(\"0\") #use zero in case a file is not found to avoid NoData rasters\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            j += 1\n",
    "                    if len(file_list) > 0:\n",
    "                        if len(expression) == 0:\n",
    "                            expression = [\"0\"]\n",
    "                        outfile = os.path.join(\n",
    "                            outpath,\n",
    "                            \"impact_{0}_{1}.tif{2}\".format(\n",
    "                                sp_name,\n",
    "                                w,\n",
    "                                \"?&gdal:co:COMPRESS=LZW&gdal:co:TILED=YES&gdal:co:BIGTIFF=YES\",\n",
    "                            ),\n",
    "                        )\n",
    "                        print(\"+\".join(expression))\n",
    "                        subprocess.call(\n",
    "                            [\n",
    "                                otbbin + \"otbcli_BandMath\",\n",
    "                                \"-out\",\n",
    "                                outfile,\n",
    "                                \"-exp\",\n",
    "                                \"+\".join(expression),\n",
    "                                \"-il\",\n",
    "                            ]\n",
    "                            + file_list\n",
    "                        )\n",
    "                else:\n",
    "                    print(\"WARNING: no file for {}\".format(sp_name))\n",
    "    if filter_matrix_csv and os.path.exists(filter_matrix_csv):\n",
    "        with open(filter_matrix_csv, newline=\"\") as csvfile_f:\n",
    "            f_matrix = csv.reader(csvfile_f, delimiter=\"\\t\", quotechar=\"|\")\n",
    "            i = 0\n",
    "            for row in f_matrix:\n",
    "                if i == 0:\n",
    "                    sp_names = [re.sub(\"[^a-zA-Z0-9\\n\\.]\", \"_\", x).lower() for x in row]\n",
    "                    i += 1\n",
    "                    print(\"number of species \" + str(len(sp_names)))\n",
    "                else:\n",
    "                    j = 0\n",
    "                    k = 0\n",
    "                    expression = []\n",
    "                    sp_file_list = []\n",
    "                    for sp in row:\n",
    "                        if j > 0:\n",
    "                            impact_file = os.path.join(\n",
    "                                outpath, \"impact_{0}_{1}.tif\".format(sp_names[j], w)\n",
    "                            )\n",
    "                            if os.path.exists(impact_file):\n",
    "                                if float(sp) > 0:\n",
    "                                    k += 1\n",
    "                                    expression.append(\"im{0}b1\".format(k))\n",
    "                                    sp_file_list.append(impact_file)\n",
    "                            else:\n",
    "                                print(j)\n",
    "                                print(\"WARNING: no file for {}\".format(sp_names[j]))\n",
    "                                expression.append(\"0\") # add zero in case of missing file to avoid nodata in further calculations\n",
    "                            j += 1\n",
    "                        else:\n",
    "                            j += 1\n",
    "                    if len(sp_file_list) > 0:\n",
    "                        outfile = os.path.join(\n",
    "                            outpath,\n",
    "                            \"cimpal_{0}_{1}.tif{2}\".format(\n",
    "                                row[0],\n",
    "                                w,\n",
    "                                \"?&gdal:co:COMPRESS=LZW&gdal:co:TILED=YES&gdal:co:BIGTIFF=YES\",\n",
    "                            ),\n",
    "                        )\n",
    "                        subprocess.call(\n",
    "                            [\n",
    "                                otbbin + \"otbcli_BandMath\",\n",
    "                                \"-out\",\n",
    "                                outfile,\n",
    "                                \"-exp\",\n",
    "                                \"+\".join(expression),\n",
    "                                \"-il\",\n",
    "                            ]\n",
    "                            + sp_file_list\n",
    "                        )\n",
    "                    else:\n",
    "                        print(\"WARNING: no file for pathway {row[0]} and matrix {w}\")\n",
    "    else:\n",
    "        k = 1\n",
    "        expression = []\n",
    "        sp_file_list = []\n",
    "        for impact_file in glob.glob(\n",
    "            os.path.join(outpath, \"impact_*_{0}.tif\".format(w))\n",
    "        ):\n",
    "            expression.append(\"im{0}b1\".format(k))\n",
    "            sp_file_list.append(impact_file)\n",
    "            k += 1\n",
    "        if len(sp_file_list) > 0:\n",
    "            outfile = os.path.join(\n",
    "                outpath,\n",
    "                \"cimpal_all_{0}.tif{1}\".format(\n",
    "                    w, \"?&gdal:co:COMPRESS=LZW&gdal:co:TILED=YES&gdal:co:BIGTIFF=YES\"\n",
    "                ),\n",
    "            )\n",
    "            subprocess.call(\n",
    "                [\n",
    "                    otbbin + \"otbcli_BandMath\",\n",
    "                    \"-out\",\n",
    "                    outfile,\n",
    "                    \"-exp\",\n",
    "                    \"+\".join(expression),\n",
    "                    \"-il\",\n",
    "                ]\n",
    "                + sp_file_list\n",
    "            )\n",
    "        else:\n",
    "            print(f\"WARNING: No impact files matrix {w}\")\n",
    "grid_srs_out = Path(outpath, \"grid_srs_toWkt.txt\")\n",
    "f = open(grid_srs_out, mode=\"w+\")\n",
    "f.write(grid_srs.ExportToWkt())\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590aca50-5ba0-4f78-a234-eea345dc25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimpal-zonal-filter\n",
    "\n",
    "gris_srs_to_Wkt_path = out_path + \"/grid_srs_toWkt.txt\"\n",
    "maps_path = out_path\n",
    "\n",
    "shps_path = zones_file\n",
    "\n",
    "import os\n",
    "\n",
    "def create_directory_if_not_exists(path):\n",
    "    if not os.path.exists(path):\n",
    "        try:\n",
    "            os.makedirs(path)\n",
    "            print(\"Directory created successfully at:\", path)\n",
    "        except OSError as e:\n",
    "            print(\"Error creating directory:\", e)\n",
    "    else:\n",
    "        print(\"Directory already exists at:\", path)\n",
    "\n",
    "\n",
    "\n",
    "create_directory_if_not_exists(f\"{conf_data_dir}/output/Zonal_out\")\n",
    "out_path2 = f\"{conf_data_dir}/output/Zonal_out\"\n",
    "\n",
    "\n",
    "species_para = True\n",
    "\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "##\"\"\"\n",
    "##LIFEWATCH_ZONAL_STATISTICS - Copyright (C) <2021>  Université catholique de Louvain (UCLouvain), Belgique,\n",
    "##\n",
    "##\n",
    "##\n",
    "##List of the contributors to the development of LIFEWATCH_CIMPAL: see LICENSE file.\n",
    "##Description and complete License: see LICENSE file.\n",
    "##\n",
    "##This program (LIFEWATCH_ZONAL_STATISTICS) is free software:\n",
    "##you can redistribute it and/or modify it under the terms of the\n",
    "##GNU General Public License as published by the Free Software\n",
    "##Foundation, either version 3 of the License, or (at your option)\n",
    "##any later version.\n",
    "##This program is distributed in the hope that it will be useful,\n",
    "##but WITHOUT ANY WARRANTY; without even the implied warranty of\n",
    "##MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n",
    "##GNU General Public License for more details.\n",
    "##You should have received a copy of the GNU General Public License\n",
    "##along with this program (see COPYING file).  If not,\n",
    "##see <http://www.gnu.org/licenses/>.\n",
    "##\"\"\"\n",
    "\n",
    "##\n",
    "import subprocess, glob\n",
    "import os\n",
    "\n",
    "from osgeo import gdal\n",
    "\n",
    "    \n",
    "from pathlib import Path\n",
    "\n",
    "import argparse\n",
    "\n",
    "os.environ['PROJ_LIB'] = '/usr/local/otb/share/proj/'\n",
    "\n",
    "# parser = argparse.ArgumentParser(description=\"Biotope Cimpal Calculator\")\n",
    "\n",
    "# parser.add_argument(\"-s\", \"--shps\", type=str, help=\"Shps Dir\", required=True)\n",
    "# parser.add_argument(\"-m\", \"--maps\", type=str, help=\"Maps Dir\", required=True)\n",
    "# parser.add_argument(\"-zf\", \"--zone_field\", type=str, help=\"Zone field id\", required=True)\n",
    "# parser.add_argument(\"-srs\", \"--gris_srs_to_Wkt\", type=str, help=\"grid srs txt\", required=True)\n",
    "# parser.add_argument(\"-o\", \"--out\", type=str, help=\"Out dir\", required=True)\n",
    "# parser.add_argument(\"-sp\", \"--species\", type=bool, help=\"Run Zonal on species files as well\", default=True)\n",
    "\n",
    "# args = parser.parse_args()\n",
    "shp_dir = shps_path\n",
    "shp_files = os.listdir(shp_dir)\n",
    "\n",
    "maps_dir = maps_path\n",
    "\n",
    "grid_srs_to_Wkt = gris_srs_to_Wkt_path\n",
    "\n",
    "if Path(grid_srs_to_Wkt).exists():\n",
    "    with open(grid_srs_to_Wkt, \"r\") as f:\n",
    "        grid_srs_to_Wkt = f.read()\n",
    "\n",
    "out = out_path2\n",
    "\n",
    "shp_file = None\n",
    "\n",
    "for file in shp_files:\n",
    "    if file.endswith(\".shp\"):\n",
    "        shp_file = os.path.join(shp_dir, file)\n",
    "        break\n",
    "\n",
    "# get input file names from orchestrator\n",
    "shp_zone = shp_file\n",
    "\n",
    "zone_field = param_zone_field_para\n",
    "zonal = Path(\"/usr/local/lw_apps/lwZonalStatistics\")\n",
    "\n",
    "# Filename of the raster Tiff that will be created\n",
    "raster_z = shp_zone[:-4] + \".tif\"\n",
    "\n",
    "tif_files = glob.glob(os.path.join(maps_dir, \"*.tif\"))\n",
    "\n",
    "#get the characteristics of the file \n",
    "hDataset = gdal.Open(tif_files[0], gdal.GA_ReadOnly)\n",
    "adfGeoTransform = hDataset.GetGeoTransform(can_return_null=True)\n",
    "\n",
    "if adfGeoTransform is not None:\n",
    "    dfGeoXUL = adfGeoTransform[0]\n",
    "    dfGeoYUL = adfGeoTransform[3]\n",
    "    dfGeoXLR = (\n",
    "        adfGeoTransform[0] + adfGeoTransform[1] * hDataset.RasterXSize + adfGeoTransform[2] * hDataset.RasterYSize\n",
    "    )\n",
    "    dfGeoYLR = (\n",
    "        adfGeoTransform[3] + adfGeoTransform[4] * hDataset.RasterXSize + adfGeoTransform[5] * hDataset.RasterYSize\n",
    "    )\n",
    "    xres = str(abs(adfGeoTransform[1]))\n",
    "    yres = str(abs(adfGeoTransform[5]))\n",
    "\n",
    "# Project to the same coordinate system than the grid\n",
    "outshp = shp_zone[:-4] + \"_gridSRS.shp\"\n",
    "\n",
    "subprocess.call([\"ogr2ogr\", \"-progress\", \"-t_srs\", grid_srs_to_Wkt, outshp, shp_zone])\n",
    "subprocess.call(\n",
    "    [\n",
    "        \"gdal_rasterize\",\n",
    "        \"-a\",\n",
    "        zone_field,\n",
    "        \"-ot\",\n",
    "        \"uint16\",\n",
    "        \"-co\",\n",
    "        \"BIGTIFF=YES\",\n",
    "        \"-co\",\n",
    "        \"TILED=YES\",\n",
    "        \"-co\",\n",
    "        \"COMPRESS=DEFLATE\",\n",
    "        \"-te\",\n",
    "        str(dfGeoXUL),\n",
    "        str(dfGeoYLR),\n",
    "        str(dfGeoXLR),\n",
    "        str(dfGeoYUL),\n",
    "        \"-tr\",\n",
    "        str(xres),\n",
    "        str(yres),\n",
    "        outshp,\n",
    "        raster_z,\n",
    "    ]\n",
    ")\n",
    "\n",
    "if not species_para:\n",
    "    tif_files =[f for f in tif_files if 'cimpal_' in f]\n",
    "\n",
    "for file in tif_files:\n",
    "    is_cimpal = True\n",
    "    print(f\"Running for: {file}\")\n",
    "    \n",
    "    tabular_out = Path(\n",
    "        out, os.path.split(file)[1][:-4] + \"_stat.csv\",\n",
    "    )\n",
    "    \n",
    "    print(f\"{str(tabular_out)}\")\n",
    "\n",
    "    subprocess.call([str(zonal), raster_z, file, tabular_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ba9b2-c6ea-4bef-94da-03d15f2c2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cimpal-upload-results\n",
    "\n",
    "import base64\n",
    "import os\n",
    "import uuid\n",
    "from minio import Minio\n",
    "\n",
    "minio_client = Minio(param_s3_server, access_key=param_s3_access_key, secret_key=param_s3_secret_key, secure=True)\n",
    "\n",
    "unique_folder = str(uuid.uuid4())\n",
    "\n",
    "for filename in os.listdir(out_path2):\n",
    "    file_path = f\"{out_path2}/{filename}\"\n",
    "    dir_basename = os.path.basename(out_path2)\n",
    "    user_prefix = param_s3_prefix_output[:-1]\n",
    "    object_name = f\"{user_prefix}/{unique_folder}/{dir_basename}/{filename}\"\n",
    "    print(\"Uploading\", file_path, \"->\", object_name)\n",
    "    minio_client.fput_object(param_s3_bucket_output, object_name=object_name, file_path=file_path)\n",
    "\n",
    "print(\n",
    "    f\"Files uploaded to https://{param_s3_server.replace('9000', '9001')}\"\n",
    "    f\"/browser\"\n",
    "    f\"/{param_s3_bucket_output}\"\n",
    "    f\"/{base64.b64encode(f'{user_prefix}/{unique_folder}/'.encode()).decode()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:biotope]",
   "language": "python",
   "name": "conda-env-biotope-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "toc-autonumbering": true,
  "toc-showcode": true,
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
